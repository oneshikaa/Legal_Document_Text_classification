{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gb3lfsf70r9_",
        "outputId": "1b5fea66-0587-4539-fce5-294f81f2f523"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqtHFhlo0h66",
        "outputId": "915372e5-2292-447e-a38a-56c7cb6ceca2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "Here, we present the results of our research and analyze them in detail. We discuss the implications and draw conclusions based on the findings. Chapter 5 Conclusion The final chapter summarizes the key points discussed in the document and offers recommendations for future research.\n",
            "\n",
            "Selected Sentences:\n",
            "['Here,', 'we', 'present', 'the', 'results', 'of', 'our', 'research', 'and', 'analyze', 'them', 'in', 'detail.', 'We', 'discuss', 'the', 'implications', 'and', 'draw', 'conclusions', 'based', 'on', 'the', 'findings.']\n",
            "['Chapter', '5']\n",
            "['Conclusion']\n",
            "['The', 'final', 'chapter', 'summarizes', 'the', 'key', 'points', 'discussed', 'in', 'the', 'document', 'and', 'offers', 'recommendations', 'for', 'future', 'research.']\n",
            "\n",
            "Retention Ratio: 0.3826086956521739\n",
            "Gist Diversity: 0.8409090909090909\n",
            "ROUGE Scores:\n",
            "Precision: 0.5\n",
            "Recall: 0.391304347826087\n",
            "F-score: 0.439024385318263\n"
          ]
        }
      ],
      "source": [
        "################# ISW\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "class Sentence:\n",
        "    def __init__(self, words, weight=0, topics=None, position=None,\n",
        "                 topics_lda_swsw=None, topics_lda_isw=None, topics_lda_rsw=None):\n",
        "        self.words = words\n",
        "        self.weight = weight\n",
        "        self.topics = topics\n",
        "        self.position = position\n",
        "        self.topics_lda_swsw = topics_lda_swsw\n",
        "        self.topics_lda_isw = topics_lda_isw\n",
        "        self.topics_lda_rsw = topics_lda_rsw\n",
        "\n",
        "def preprocess_document(document):\n",
        "    # Split the document into sentences\n",
        "    sentences = document.split('\\n')\n",
        "    # Remove empty lines and leading/trailing whitespaces\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "    # Tokenize each sentence into words\n",
        "    processed_document = [Sentence(sentence.split()) for sentence in sentences]\n",
        "    return processed_document\n",
        "\n",
        "def compute_topic_words(document, num_topics, num_iterations):\n",
        "    # Compute the topic words using LDA\n",
        "    sentences = [' '.join(sentence.words) for sentence in document]\n",
        "    vectorizer = CountVectorizer()\n",
        "    document_vectors = vectorizer.fit_transform(sentences)\n",
        "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42, n_jobs=-1)\n",
        "    lda.fit(document_vectors)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    topic_words = []\n",
        "    for topic_idx, topic in enumerate(lda.components_):\n",
        "        top_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n",
        "        topic_words.append(top_words)\n",
        "    return topic_words\n",
        "\n",
        "\n",
        "def compute_sentence_weights(sentences, topic_words):\n",
        "    # Compute the LDA-ISW weights for each sentence\n",
        "    weights = []\n",
        "    total_topic_word_frequency = sum(len(words) for words in topic_words)  # Total frequency of all topic words\n",
        "    for sentence in sentences:\n",
        "        sentence_weight = sum(word in topic_words for word in sentence.words) / total_topic_word_frequency\n",
        "        weights.append(sentence_weight)\n",
        "    return weights\n",
        "\n",
        "def compute_redundancy_rate(selected_sentences):\n",
        "    # Compute the redundancy rate of the selected sentences\n",
        "    unique_sentences = set([sentence.words for sentence in selected_sentences])\n",
        "    return 1 - (len(unique_sentences) / len(selected_sentences))\n",
        "\n",
        "\n",
        "def compute_inclusive_topic_diversity(selected_sentences):\n",
        "    # Compute the inclusive topic diversity of the selected sentences\n",
        "    unique_topics = set([topic for sentence in selected_sentences for topic in sentence.topics])\n",
        "    return len(unique_topics) / len(selected_sentences)\n",
        "\n",
        "\n",
        "def compute_exclusive_topic_diversity(selected_sentences):\n",
        "     # Compute the exclusive topic diversity of the selected sentences\n",
        "    unique_topics = set([topic for sentence in selected_sentences for topic in sentence.topics])\n",
        "    total_topics = set([topic for sentence in selected_sentences for topic in sentence.topics_lda_isw])\n",
        "    return len(unique_topics) / len(total_topics)\n",
        "\n",
        "def select_sentences(processed_document, sentence_weights, compression_ratio, redundancy_threshold,\n",
        "                     inclusive_topic_diversity_threshold, exclusive_topic_diversity_threshold):\n",
        "\n",
        "    num_selected_sentences = max(1, int(compression_ratio * len(processed_document)))\n",
        "    selected_indices = np.argsort(sentence_weights)[-num_selected_sentences:]\n",
        "    selected_indices.sort()\n",
        "    selected_sentences = [processed_document[idx] for idx in selected_indices]\n",
        "\n",
        "    num_sentences = len(processed_document)\n",
        "    sorted_indices = np.argsort(sentence_weights)[::-1]\n",
        "    # selected_sentences = []\n",
        "    cumulative_length = 0\n",
        "    for index in sorted_indices:\n",
        "        sentence = processed_document[index]\n",
        "        if (cumulative_length + len(sentence.words)) / num_sentences > compression_ratio:\n",
        "            break\n",
        "        if compute_redundancy_rate(selected_sentences + [sentence]) <= redundancy_threshold \\\n",
        "                and compute_inclusive_topic_diversity(selected_sentences + [sentence]) >= inclusive_topic_diversity_threshold \\\n",
        "                and compute_exclusive_topic_diversity(selected_sentences + [sentence]) <= exclusive_topic_diversity_threshold:\n",
        "            selected_sentences.append(sentence)\n",
        "            cumulative_length += len(sentence.words)\n",
        "    return selected_sentences\n",
        "\n",
        "\n",
        "def generate_summary(selected_sentences, smoothing_factor):\n",
        "    # Sort the selected sentences based on their position\n",
        "    sorted_sentences = sorted(selected_sentences, key=lambda x: x.position if x.position is not None else float('inf'))\n",
        "    # Apply summary smoothing\n",
        "    smoothed_sentences = []\n",
        "    prev_weight = None\n",
        "    for sentence in sorted_sentences:\n",
        "        if prev_weight is None or sentence.weight >= smoothing_factor * prev_weight:\n",
        "            smoothed_sentences.append(sentence)\n",
        "            prev_weight = sentence.weight\n",
        "    # Generate the summary by joining the words of the selected sentences\n",
        "    summary = ' '.join([' '.join(sentence.words) for sentence in smoothed_sentences])\n",
        "    return summary\n",
        "\n",
        "\n",
        "def calculate_retention_ratio(selected_sentences, processed_document):\n",
        "    # Calculate the retention ratio of the selected sentences\n",
        "    num_selected_words = sum(len(sentence.words) for sentence in selected_sentences)\n",
        "    num_total_words = sum(len(sentence.words) for sentence in processed_document)\n",
        "    return num_selected_words / num_total_words\n",
        "\n",
        "\n",
        "def calculate_gist_diversity(selected_sentences):\n",
        "    # Calculate the gist diversity of the selected sentences\n",
        "    num_selected_words = sum(len(sentence.words) for sentence in selected_sentences)\n",
        "    num_unique_words = len(set([word for sentence in selected_sentences for word in sentence.words]))\n",
        "    return num_unique_words / num_selected_words\n",
        "\n",
        "\n",
        "def summarize_document(document, num_topics, num_iterations, compression_ratio, redundancy_threshold,\n",
        "                       inclusive_topic_diversity_threshold, exclusive_topic_diversity_threshold,\n",
        "                       summary_smoothing_factor):\n",
        "    processed_document = preprocess_document(document)\n",
        "    topic_words = compute_topic_words(processed_document, num_topics, num_iterations)\n",
        "    sentence_weights = compute_sentence_weights(processed_document, topic_words)\n",
        "    selected_sentences = select_sentences(processed_document, sentence_weights, compression_ratio,\n",
        "                                           redundancy_threshold, inclusive_topic_diversity_threshold,\n",
        "                                           exclusive_topic_diversity_threshold)\n",
        "    retention_ratio = calculate_retention_ratio(selected_sentences, processed_document)\n",
        "    gist_diversity = calculate_gist_diversity(selected_sentences)\n",
        "    summary = generate_summary(selected_sentences, summary_smoothing_factor)\n",
        "    return summary, selected_sentences, retention_ratio, gist_diversity\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "\n",
        "document = \"\"\"\n",
        "Chapter 1\n",
        "Introduction\n",
        "This is the first chapter of the document. It provides an overview of the topic and introduces key concepts.\n",
        "\n",
        "Chapter 2\n",
        "Literature Review\n",
        "In this chapter, we review existing literature on the topic. We discuss various studies and their findings.\n",
        "\n",
        "Chapter 3\n",
        "Methodology\n",
        "This chapter describes the methodology used in the research. It explains the data collection process and the analytical techniques employed.\n",
        "\n",
        "Chapter 4\n",
        "Results and Analysis\n",
        "Here, we present the results of our research and analyze them in detail. We discuss the implications and draw conclusions based on the findings.\n",
        "\n",
        "Chapter 5\n",
        "Conclusion\n",
        "The final chapter summarizes the key points discussed in the document and offers recommendations for future research.\n",
        "\"\"\"\n",
        "\n",
        "num_topics = 5\n",
        "num_iterations = 100\n",
        "compression_ratio = 0.3\n",
        "redundancy_threshold = 0.3\n",
        "inclusive_topic_diversity_threshold = 0.5\n",
        "exclusive_topic_diversity_threshold = 0.2\n",
        "summary_smoothing_factor = 0.8\n",
        "\n",
        "summary, selected_sentences, retention_ratio, gist_diversity = summarize_document(\n",
        "    document, num_topics, num_iterations, compression_ratio, redundancy_threshold,\n",
        "    inclusive_topic_diversity_threshold, exclusive_topic_diversity_threshold, summary_smoothing_factor\n",
        ")\n",
        "\n",
        "print(\"Summary:\")\n",
        "print(summary)\n",
        "\n",
        "print(\"\\nSelected Sentences:\")\n",
        "for sentence in selected_sentences:\n",
        "    print(sentence.words)\n",
        "\n",
        "print(\"\\nRetention Ratio:\", retention_ratio)\n",
        "print(\"Gist Diversity:\", gist_diversity)\n",
        "\n",
        "from rouge import Rouge\n",
        "\n",
        "def compute_rouge_scores(summary, reference):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(summary, reference)[0]\n",
        "    precision = scores['rouge-1']['p']\n",
        "    recall = scores['rouge-1']['r']\n",
        "    f_score = scores['rouge-1']['f']\n",
        "    return precision, recall, f_score\n",
        "\n",
        "reference = \"\"\"\n",
        "Chapter 1 provides an introduction and overview of the topic, while\n",
        "Chapter 2 focuses on reviewing existing literature.\n",
        "Chapter 3 describes the methodology used in the research, including data collection and analytical techniques.\n",
        "Chapter 4 presents the research results and analysis, discussing implications and conclusions. Finally,\n",
        "Chapter 5 summarizes the key points and offers recommendations for future research.\n",
        "\"\"\"\n",
        "\n",
        "precision, recall, f_score = compute_rouge_scores(summary, reference)\n",
        "\n",
        "print(\"ROUGE Scores:\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F-score: {f_score}\")\n"
      ]
    }
  ]
}