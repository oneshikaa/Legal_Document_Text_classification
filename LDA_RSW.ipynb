{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j56fqP6G1tMn",
        "outputId": "e1ba0a68-19e0-444d-c6fc-c086029d07d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################## RSW\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "class Sentence:\n",
        "    def __init__(self, words, weight=0, topics=None, position=None,\n",
        "                 topics_lda_swsw=None, topics_lda_isw=None, topics_lda_rsw=None):\n",
        "        self.words = words\n",
        "        self.weight = weight\n",
        "        self.topics = topics\n",
        "        self.position = position\n",
        "        self.topics_lda_swsw = topics_lda_swsw\n",
        "        self.topics_lda_isw = topics_lda_isw\n",
        "        self.topics_lda_rsw = topics_lda_rsw\n",
        "\n",
        "def preprocess_document(document):\n",
        "    # Split the document into sentences\n",
        "    sentences = document.split('\\n')\n",
        "    # Remove empty lines and leading/trailing whitespaces\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "    # Tokenize each sentence into words\n",
        "    processed_document = [Sentence(sentence.split()) for sentence in sentences]\n",
        "    return processed_document\n",
        "\n",
        "def compute_topic_words(document, num_topics, num_iterations):\n",
        "    # Compute the topic words using LDA\n",
        "    sentences = [' '.join(sentence.words) for sentence in document]\n",
        "    vectorizer = CountVectorizer()\n",
        "    document_vectors = vectorizer.fit_transform(sentences)\n",
        "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42, n_jobs=-1)\n",
        "    lda.fit(document_vectors)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    topic_words = []\n",
        "    for topic_idx, topic in enumerate(lda.components_):\n",
        "        top_words = [feature_names[i] for i in topic.argsort()[:-11:-1]]\n",
        "        topic_words.append(top_words)\n",
        "    return topic_words\n",
        "\n",
        "def compute_sentence_topic_word_frequency(sentences, topic_words):\n",
        "    # Compute the topic-word frequency for each sentence\n",
        "    for sentence in sentences:\n",
        "        sentence.topics_lda_rsw = [0] * len(topic_words)\n",
        "        for word in sentence.words:\n",
        "            for topic_idx, topic in enumerate(topic_words):\n",
        "                if word in topic:\n",
        "                    sentence.topics_lda_rsw[topic_idx] += 1\n",
        "\n",
        "def compute_sentence_weights(sentences):\n",
        "    # Compute the LDA-RSW weights for each sentence\n",
        "    weights = [sum(sentence.topics_lda_rsw) for sentence in sentences]\n",
        "    total_weight = sum(weights)\n",
        "    for sentence in sentences:\n",
        "        sentence.weight = sentence.weight / total_weight if total_weight > 0 else 0\n",
        "    return weights\n",
        "\n",
        "def compute_redundancy_rate(selected_sentences):\n",
        "    # Compute the redundancy rate of selected sentences\n",
        "    total_words = sum([len(sentence.words) for sentence in selected_sentences])\n",
        "    unique_words = len(set([word for sentence in selected_sentences for word in sentence.words]))\n",
        "\n",
        "    redundancy_rate = 0\n",
        "    if total_words > 0:\n",
        "        redundancy_rate = (total_words - unique_words) / total_words\n",
        "\n",
        "    return redundancy_rate\n",
        "\n",
        "def compute_inclusive_topic_diversity(selected_sentences):\n",
        "    # Compute the inclusive topic diversity of selected sentences\n",
        "    topic_indices = set()\n",
        "    for sentence in selected_sentences:\n",
        "        if sentence.topics:\n",
        "            topic_indices.update(sentence.topics)\n",
        "    inclusive_topic_diversity = len(topic_indices) / len(selected_sentences)\n",
        "    return inclusive_topic_diversity\n",
        "\n",
        "\n",
        "def compute_exclusive_topic_diversity(selected_sentences):\n",
        "    # Compute the exclusive topic diversity of selected sentences\n",
        "    topic_counts = [0] * len(selected_sentences[0].topics_lda_rsw)\n",
        "    for sentence in selected_sentences:\n",
        "        for topic_idx, topic_count in enumerate(sentence.topics_lda_rsw):\n",
        "            if topic_count > 0:\n",
        "                topic_counts[topic_idx] += 1\n",
        "    exclusive_topic_diversity = np.mean(topic_counts) / len(selected_sentences)\n",
        "    return exclusive_topic_diversity\n",
        "\n",
        "def select_sentences(sentences, weights, compression_ratio, redundancy_threshold, inclusive_topic_diversity_threshold, exclusive_topic_diversity_threshold):\n",
        "    # Sort the sentences based on the weights\n",
        "    num_selected_sentences = max(1, int(compression_ratio * len(sentences)))\n",
        "    selected_indices = np.argsort(weights)[-num_selected_sentences:]\n",
        "    selected_indices.sort()\n",
        "    selected_sentences = [sentences[idx] for idx in selected_indices]\n",
        "\n",
        "    sorted_indices = np.argsort(weights)\n",
        "    sorted_indices = sorted_indices.tolist()  # Convert the NumPy array to a Python list\n",
        "    sorted_indices.reverse()\n",
        "\n",
        "    while len(selected_sentences) < num_selected_sentences and sorted_indices:\n",
        "        idx = sorted_indices.pop(0)\n",
        "        selected_indices.append(idx)\n",
        "        selected_sentences.append(sentences[idx])\n",
        "\n",
        "        # Apply redundancy, inclusive topic diversity, and exclusive topic diversity checks\n",
        "        if len(selected_sentences) > 1:\n",
        "            # redundant = check_redundancy(selected_sentences, redundancy_threshold)\n",
        "            redundant = compute_redundancy_rate(selected_sentences)\n",
        "            inclusive_topic_diversity = compute_inclusive_topic_diversity(selected_sentences)\n",
        "            exclusive_topic_diversity = compute_exclusive_topic_diversity(selected_sentences)\n",
        "\n",
        "            if redundant or inclusive_topic_diversity < inclusive_topic_diversity_threshold or exclusive_topic_diversity > exclusive_topic_diversity_threshold:\n",
        "                selected_indices.pop(0)\n",
        "                selected_sentences.pop(0)\n",
        "\n",
        "    return selected_sentences\n",
        "\n",
        "\n",
        "def calculate_retention_ratio(selected_sentences, document):\n",
        "    # Calculate the retention ratio of the selected sentences\n",
        "    selected_words = set([word for sentence in selected_sentences for word in sentence.words])\n",
        "    total_words = set([word for sentence in document for word in sentence.words])\n",
        "    retention_ratio = len(selected_words) / len(total_words)\n",
        "    return retention_ratio\n",
        "\n",
        "def calculate_gist_diversity(selected_sentences):\n",
        "    # Calculate the gist diversity of the selected sentences\n",
        "    sentence_lengths = [len(sentence.words) for sentence in selected_sentences]\n",
        "    avg_length = np.mean(sentence_lengths)\n",
        "    deviation = np.sqrt(np.mean((sentence_lengths - avg_length) ** 2))\n",
        "    gist_diversity = deviation / avg_length\n",
        "    return gist_diversity\n",
        "\n",
        "def generate_summary(sentences):\n",
        "    # Generate the summary by concatenating the selected sentences\n",
        "    summary = ' '.join([' '.join(sentence.words) for sentence in sentences])\n",
        "    return summary\n",
        "\n",
        "def smooth_summary(selected_sentences, num_iterations):\n",
        "    # Smooth the summary by iteratively adding highly diverse and important sentences\n",
        "    summary = []\n",
        "    for _ in range(num_iterations):\n",
        "        summary.extend(selected_sentences)\n",
        "        topic_words = compute_topic_words(summary, num_topics, num_iterations)\n",
        "        compute_sentence_topic_word_frequency(selected_sentences, topic_words)\n",
        "        sentence_weights = compute_sentence_weights(selected_sentences)\n",
        "        selected_sentences = select_sentences(selected_sentences, sentence_weights, 1.0, 0.0, 0.0, 1.0)\n",
        "    return summary\n",
        "\n",
        "def summarize_document(document, num_topics, num_iterations, compression_ratio,\n",
        "                       redundancy_threshold, inclusive_topic_diversity_threshold,\n",
        "                       exclusive_topic_diversity_threshold, smoothing_iterations):\n",
        "    processed_document = preprocess_document(document)\n",
        "    topic_words = compute_topic_words(processed_document, num_topics, num_iterations)\n",
        "    compute_sentence_topic_word_frequency(processed_document, topic_words)\n",
        "    sentence_weights = compute_sentence_weights(processed_document)\n",
        "    selected_sentences = select_sentences(processed_document, sentence_weights, compression_ratio,\n",
        "                                           redundancy_threshold, inclusive_topic_diversity_threshold,\n",
        "                                           exclusive_topic_diversity_threshold)\n",
        "    summary = generate_summary(selected_sentences)\n",
        "    smoothed_summary = smooth_summary(selected_sentences, smoothing_iterations)\n",
        "    gist_diversity = calculate_gist_diversity(selected_sentences)\n",
        "    retention_ratio = calculate_retention_ratio(selected_sentences, processed_document)\n",
        "    return summary, smoothed_summary, gist_diversity, retention_ratio\n",
        "\n",
        "# Example Usage\n",
        "\n",
        "document = \"\"\"\n",
        "Chapter 1\n",
        "Introduction\n",
        "This is the first chapter of the document. It provides an overview of the topic and introduces key concepts.\n",
        "\n",
        "Chapter 2\n",
        "Literature Review\n",
        "In this chapter, we review existing literature on the topic. We discuss various studies and their findings.\n",
        "\n",
        "Chapter 3\n",
        "Methodology\n",
        "This chapter describes the methodology used in the research. It explains the data collection process and the analytical techniques employed.\n",
        "\n",
        "Chapter 4\n",
        "Results and Analysis\n",
        "Here, we present the results of our research and analyze them in detail. We discuss the implications and draw conclusions based on the findings.\n",
        "\n",
        "Chapter 5\n",
        "Conclusion\n",
        "The final chapter summarizes the key points discussed in the document and offers recommendations for future research.\n",
        "\"\"\"\n",
        "\n",
        "num_topics = 5\n",
        "num_iterations = 100\n",
        "compression_ratio = 0.3\n",
        "redundancy_threshold = 0.2\n",
        "inclusive_topic_diversity_threshold = 0.6\n",
        "exclusive_topic_diversity_threshold = 0.4\n",
        "smoothing_iterations = 5\n",
        "\n",
        "summary, smoothed_summary, gist_diversity, retention_ratio = summarize_document(document, num_topics, num_iterations,\n",
        "                                                                                 compression_ratio, redundancy_threshold,\n",
        "                                                                                 inclusive_topic_diversity_threshold,\n",
        "                                                                                 exclusive_topic_diversity_threshold,\n",
        "                                                                                 smoothing_iterations)\n",
        "\n",
        "print(\"Summary:\")\n",
        "print(summary)\n",
        "\n",
        "print(\"\\nGist Diversity:\", gist_diversity)\n",
        "print(\"Retention Ratio:\", retention_ratio)\n",
        "\n",
        "from rouge import Rouge\n",
        "\n",
        "# Function to calculate precision, recall, and F-score for ROUGE scores\n",
        "def calculate_rouge_metrics(summary, reference):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(summary, reference)\n",
        "    precision = scores[0]['rouge-1']['p']\n",
        "    recall = scores[0]['rouge-1']['r']\n",
        "    f_score = scores[0]['rouge-1']['f']\n",
        "    return precision, recall, f_score\n",
        "\n",
        "reference = \"\"\"\n",
        "Chapter 1 provides an introduction and overview of the topic, while\n",
        "Chapter 2 focuses on reviewing existing literature.\n",
        "Chapter 3 describes the methodology used in the research, including data collection and analytical techniques.\n",
        "Chapter 4 presents the research results and analysis, discussing implications and conclusions. Finally,\n",
        "Chapter 5 summarizes the key points and offers recommendations for future research.\n",
        "\"\"\"\n",
        "# Calculate ROUGE metrics\n",
        "precision, recall, f_score = calculate_rouge_metrics(summary, reference)\n",
        "\n",
        "# Print the metrics\n",
        "print()\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F-score:\", f_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7712V4fp10JP",
        "outputId": "2bcde8a1-897f-465a-c4a9-110b499e7719"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "This is the first chapter of the document. It provides an overview of the topic and introduces key concepts. This chapter describes the methodology used in the research. It explains the data collection process and the analytical techniques employed. Here, we present the results of our research and analyze them in detail. We discuss the implications and draw conclusions based on the findings. The final chapter summarizes the key points discussed in the document and offers recommendations for future research.\n",
            "\n",
            "Gist Diversity: 0.1274754878398196\n",
            "Retention Ratio: 0.7051282051282052\n",
            "\n",
            "Precision: 0.49056603773584906\n",
            "Recall: 0.5652173913043478\n",
            "F-score: 0.5252525202775227\n"
          ]
        }
      ]
    }
  ]
}