{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exRa6mhvR2dW",
        "outputId": "49acc64e-fefc-4e5e-b852-a48ad4395ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################### SWSW\n",
        "\n",
        "# Step 1: Text Preprocessing\n",
        "\n",
        "def chapter_segmentation(document):\n",
        "    # Split the document into chapters\n",
        "    chapters = document.split('\\n\\nChapter ')[1:]\n",
        "    chapters = ['Chapter ' + chapter for chapter in chapters]\n",
        "    return chapters\n",
        "\n",
        "def sentence_segmentation(chapter):\n",
        "    # Split the chapter into sentences and store their positions\n",
        "    sentences = chapter.split('. ')\n",
        "    sentences = [(sentence + '.', i) for i, sentence in enumerate(sentences)]\n",
        "    return sentences\n",
        "\n",
        "def word_tokenization(sentence):\n",
        "    # Tokenize the sentence into words\n",
        "    words = sentence.split()\n",
        "    return words\n",
        "\n",
        "def remove_punctuation(words):\n",
        "    # Remove punctuation from words\n",
        "    punctuation = ['.', ',', ';', ':', '!', '?', '\"', \"'\"]\n",
        "    words = [word for word in words if word not in punctuation]\n",
        "    return words\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    # Remove stopwords from words\n",
        "    stopwords = ['the', 'a', 'an', 'in', 'on', 'of', 'to', 'for', 'by', 'with']\n",
        "    words = [word for word in words if word.lower() not in stopwords]\n",
        "    return words\n",
        "\n",
        "def perform_stemming(words):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    return stemmed_words\n",
        "\n",
        "def preprocess_document(document):\n",
        "    chapters = chapter_segmentation(document)\n",
        "    processed_sentences = []\n",
        "\n",
        "    for chapter in chapters:\n",
        "        sentences = sentence_segmentation(chapter)\n",
        "\n",
        "        for sentence, position in sentences:\n",
        "            words = word_tokenization(sentence)\n",
        "            words = remove_punctuation(words)\n",
        "            words = remove_stopwords(words)\n",
        "            stemmed_words = perform_stemming(words)\n",
        "\n",
        "            processed_sentences.append(Sentence(stemmed_words, position=position))\n",
        "\n",
        "    return processed_sentences\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "class Sentence:\n",
        "    def __init__(self, words, weight=0, topics=None, position=None,\n",
        "                 topics_lda_swsw=None, topics_lda_isw=None, topics_lda_rsw=None):\n",
        "        self.words = words\n",
        "        self.weight = weight\n",
        "        self.topics = topics\n",
        "        self.position = position\n",
        "        self.topics_lda_swsw = topics_lda_swsw\n",
        "        self.topics_lda_isw = topics_lda_isw\n",
        "        self.topics_lda_rsw = topics_lda_rsw\n",
        "\n",
        "def compute_sentence_weights(sentences, window_size):\n",
        "  # Compute the LDA-SWSW weights for each sentence\n",
        "    corpus = [' '.join(sentence.words) for sentence in sentences]\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    words = vectorizer.get_feature_names_out()\n",
        "\n",
        "    lda = LatentDirichletAllocation(n_components=1)\n",
        "    lda.fit(X)\n",
        "    topic_word_distribution = lda.components_ / lda.components_.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    weights = []\n",
        "    for sentence in sentences:\n",
        "        num_words = len(sentence.words)\n",
        "        if num_words <= window_size:\n",
        "            sentence.weight = 1.0\n",
        "        else:\n",
        "            num_windows = num_words - window_size + 1\n",
        "            window_weights = []\n",
        "            for i in range(num_windows):\n",
        "                window = sentence.words[i: i + window_size]\n",
        "                num_feature_words = len(set(window).intersection(words))\n",
        "                window_weight = num_feature_words / window_size\n",
        "                window_weights.append(window_weight)\n",
        "            sentence.weight = max(window_weights)\n",
        "        weights.append(sentence.weight)\n",
        "\n",
        "    # Create a word-to-index dictionary\n",
        "    word_to_index = {word: index for index, word in enumerate(words)}\n",
        "\n",
        "    # Assign topics to sentences based on LDA\n",
        "    for sentence in sentences:\n",
        "        sentence.topics = [np.argmax(topic_word_distribution[:,word_to_index.get(word, -1)])\n",
        "                           for word in sentence.words]\n",
        "\n",
        "    return weights\n",
        "\n",
        "\n",
        "\n",
        "def calculate_redundancy_rate(selected_sentences):\n",
        "    # Calculate the redundancy rate of the selected sentences\n",
        "    all_words = []\n",
        "    unique_words = set()\n",
        "    for sentence in selected_sentences:\n",
        "        all_words.extend(sentence.words)\n",
        "        unique_words.update(sentence.words)\n",
        "    num_all_words = len(all_words)\n",
        "    num_unique_words = len(unique_words)\n",
        "    redundancy_rate = 1.0 - (num_unique_words / num_all_words)\n",
        "    return redundancy_rate\n",
        "\n",
        "def calculate_inclusive_topic_diversity(selected_sentences):\n",
        "    # Calculate the inclusive topic diversity of the selected sentences\n",
        "    topics = set()\n",
        "    for sentence in selected_sentences:\n",
        "        if sentence.topics:\n",
        "            topics.update(sentence.topics)\n",
        "    num_selected_sentences = len(selected_sentences)\n",
        "    num_topics = len(topics)\n",
        "    if num_selected_sentences == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return num_topics / num_selected_sentences\n",
        "\n",
        "def calculate_exclusive_topic_diversity(selected_sentences):\n",
        "    # Calculate the exclusive topic diversity of the selected sentences\n",
        "    topics = []\n",
        "    for sentence in selected_sentences:\n",
        "        if sentence.topics:\n",
        "            topics.extend(sentence.topics)\n",
        "    num_selected_sentences = len(selected_sentences)\n",
        "    num_topics = len(set(topics))\n",
        "    if num_selected_sentences == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return num_topics / num_selected_sentences\n",
        "\n",
        "def select_sentences(sentences, weights, compression_ratio, redundancy_threshold,\n",
        "                     inclusive_topic_diversity_threshold, exclusive_topic_diversity_threshold):\n",
        "    # Select the top sentences based on the weights and the compression ratio\n",
        "    num_selected_sentences = max(1, int(compression_ratio * len(sentences)))\n",
        "    selected_indices = np.argsort(weights)[-num_selected_sentences:]\n",
        "    selected_indices.sort()\n",
        "    selected_sentences = [sentences[idx] for idx in selected_indices]\n",
        "\n",
        "    sorted_indices = np.argsort(weights)[::-1]\n",
        "    current_summary = []\n",
        "    current_redundancy_rate = 0.0\n",
        "    current_inclusive_topic_diversity = 0.0\n",
        "    current_exclusive_topic_diversity = 0.0\n",
        "\n",
        "    for idx in sorted_indices:\n",
        "        sentence = sentences[idx]\n",
        "        current_summary.append(sentence)\n",
        "        current_redundancy_rate = calculate_redundancy_rate(current_summary)\n",
        "        current_inclusive_topic_diversity = calculate_inclusive_topic_diversity(current_summary)\n",
        "        current_exclusive_topic_diversity = calculate_exclusive_topic_diversity(current_summary)\n",
        "\n",
        "        if (current_redundancy_rate <= redundancy_threshold and\n",
        "                current_inclusive_topic_diversity >= inclusive_topic_diversity_threshold and\n",
        "                current_exclusive_topic_diversity <= exclusive_topic_diversity_threshold):\n",
        "            selected_indices.append(idx)\n",
        "            selected_sentences.append(sentence)\n",
        "\n",
        "        if len(selected_sentences) >= int(compression_ratio * len(sentences)):\n",
        "            break\n",
        "\n",
        "    return selected_sentences\n",
        "\n",
        "def calculate_gist_similarity(sentence1, sentence2):\n",
        "    # Calculate the Gist similarity between two sentences\n",
        "    words1 = set(sentence1.words)\n",
        "    words2 = set(sentence2.words)\n",
        "    intersection = words1.intersection(words2)\n",
        "    union = words1.union(words2)\n",
        "    similarity = len(intersection) / len(union)\n",
        "    return similarity\n",
        "\n",
        "\n",
        "def calculate_gist_diversity(selected_sentences):\n",
        "    # Calculate the gist diversity of the selected sentences\n",
        "    num_sentences = len(selected_sentences)\n",
        "    if num_sentences <= 1:\n",
        "        return 0.0\n",
        "    else:\n",
        "        pairwise_gist_scores = []\n",
        "        for i in range(num_sentences - 1):\n",
        "            for j in range(i + 1, num_sentences):\n",
        "                sentence1 = selected_sentences[i]\n",
        "                sentence2 = selected_sentences[j]\n",
        "                gist_score = calculate_gist_similarity(sentence1, sentence2)\n",
        "                pairwise_gist_scores.append(gist_score)\n",
        "        mean_gist_score = np.mean(pairwise_gist_scores)\n",
        "        return mean_gist_score\n",
        "\n",
        "def calculate_retention_ratio(selected_sentences, original_document):\n",
        "    # Calculate the retention ratio of the selected sentences\n",
        "    selected_words = set()\n",
        "    for sentence in selected_sentences:\n",
        "        selected_words.update(sentence.words)\n",
        "    original_words = set()\n",
        "    for sentence in original_document:\n",
        "        original_words.update(sentence.words)\n",
        "    num_selected_words = len(selected_words)\n",
        "    num_original_words = len(original_words)\n",
        "    if num_original_words == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return num_selected_words / num_original_words\n",
        "\n",
        "def generate_summary(sentences):\n",
        "    # Generate the summary by concatenating the selected sentences\n",
        "    summary = ' '.join([' '.join(sentence.words) for sentence in sentences])\n",
        "    return summary\n",
        "\n",
        "def summarize_document(document, window_size, compression_ratio, redundancy_threshold,\n",
        "                       inclusive_topic_diversity_threshold, exclusive_topic_diversity_threshold):\n",
        "    processed_document = preprocess_document(document)\n",
        "    sentence_weights = compute_sentence_weights(processed_document, window_size)\n",
        "    selected_sentences = select_sentences(processed_document, sentence_weights, compression_ratio,\n",
        "                                           redundancy_threshold, inclusive_topic_diversity_threshold,\n",
        "                                           exclusive_topic_diversity_threshold)\n",
        "    summary = generate_summary(selected_sentences)\n",
        "    return summary, selected_sentences\n",
        "\n",
        "# Example Usage\n",
        "\n",
        "document = \"\"\"\n",
        "Chapter 1\n",
        "Introduction\n",
        "This is the first chapter of the document. It provides an overview of the topic and introduces key concepts.\n",
        "\n",
        "Chapter 2\n",
        "Literature Review\n",
        "In this chapter, we review existing literature on the topic. We discuss various studies and their findings.\n",
        "\n",
        "Chapter 3\n",
        "Methodology\n",
        "This chapter describes the methodology used in the research. It explains the data collection process and the analytical techniques employed.\n",
        "\n",
        "Chapter 4\n",
        "Results and Analysis\n",
        "Here, we present the results of our research and analyze them in detail. We discuss the implications and draw conclusions based on the findings.\n",
        "\n",
        "Chapter 5\n",
        "Conclusion\n",
        "The final chapter summarizes the key points discussed in the document and offers recommendations for future research.\n",
        "\"\"\"\n",
        "\n",
        "reference = \"\"\"\n",
        "Chapter 1 provides an introduction and overview of the topic, while\n",
        "Chapter 2 focuses on reviewing existing literature.\n",
        "Chapter 3 describes the methodology used in the research, including data collection and analytical techniques.\n",
        "Chapter 4 presents the research results and analysis, discussing implications and conclusions. Finally,\n",
        "Chapter 5 summarizes the key points and offers recommendations for future research.\n",
        "\"\"\"\n",
        "\n",
        "window_size = 8\n",
        "compression_ratio = 0.3\n",
        "redundancy_threshold = 0.3\n",
        "inclusive_topic_diversity_threshold = 0.4\n",
        "exclusive_topic_diversity_threshold = 0.2\n",
        "\n",
        "summary, selected_sentences = summarize_document(document, window_size, compression_ratio,\n",
        "                                                 redundancy_threshold, inclusive_topic_diversity_threshold,\n",
        "                                                 exclusive_topic_diversity_threshold)\n",
        "\n",
        "print(\"Summary:\")\n",
        "print(summary)\n",
        "\n",
        "print(\"\\nSelected Sentences:\")\n",
        "for sentence in selected_sentences:\n",
        "    print(sentence.words)\n",
        "\n",
        "from rouge import Rouge\n",
        "\n",
        "def calculate_rouge_scores(summary, reference):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(summary, reference)[0]\n",
        "    precision = scores['rouge-1']['p']\n",
        "    recall = scores['rouge-1']['r']\n",
        "    f_score = scores['rouge-1']['f']\n",
        "    return precision, recall, f_score\n",
        "\n",
        "\n",
        "precision, recall, f_score = calculate_rouge_scores(summary, reference)\n",
        "gist_diversity = calculate_gist_diversity(selected_sentences)\n",
        "retention_ratio = calculate_retention_ratio(selected_sentences, preprocess_document(document))\n",
        "\n",
        "print(\"\\nROUGE Scores:\")\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F-score:\", f_score)\n",
        "\n",
        "print(\"Gist Diversity:\", gist_diversity)\n",
        "print(\"Retention Ratio:\", retention_ratio)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aABRz32ER3qM",
        "outputId": "48a73fb6-9afa-4606-8dcd-b4f4537e52f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "we discuss implic and draw conclus base findings.. chapter 5 conclus final chapter summar key point discuss document and offer recommend futur research.\n",
            "\n",
            "Selected Sentences:\n",
            "['we', 'discuss', 'implic', 'and', 'draw', 'conclus', 'base', 'findings..']\n",
            "['chapter', '5', 'conclus', 'final', 'chapter', 'summar', 'key', 'point', 'discuss', 'document', 'and', 'offer', 'recommend', 'futur', 'research.']\n",
            "\n",
            "ROUGE Scores:\n",
            "Precision: 0.21052631578947367\n",
            "Recall: 0.08695652173913043\n",
            "F-score: 0.12307691893964512\n",
            "Gist Diversity: 0.15789473684210525\n",
            "Retention Ratio: 0.37254901960784315\n"
          ]
        }
      ]
    }
  ]
}